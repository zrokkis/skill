# æ–‡ä»¶å: ag_indexer.py
import os, pickle
import re
from sentence_transformers import SentenceTransformer

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# æ‰©å±•æ‰«æè·¯å¾„ï¼šæ”¯æŒå¤šä¸ªèµ„äº§ç›®å½•
ASSETS_PATHS = [
    os.path.abspath(os.path.join(CURRENT_DIR, "../../assets/expert_frameworks")),
    os.path.abspath(os.path.join(ROOT_DIR := os.path.dirname(os.path.dirname(CURRENT_DIR)), "ç²¾é€‰çŸ¥è¯†åº“"))
]
CACHE_FILE = os.path.join(CURRENT_DIR, "skills_cache.pkl")

# å‡çº§ä¸º SOTA çº§çš„ BGE-M3 æ¨¡åž‹ (1024 ç»´åº¦)
MODEL_NAME = 'BAAI/bge-m3'

def get_model_path():
    """ä¼˜å…ˆçº§: çŽ¯å¢ƒå˜é‡ -> å†…ç½®ç›®å½• -> è¿œç¨‹"""
    env_path = os.environ.get("PEER_MODEL_PATH")
    if env_path and os.path.exists(env_path): return env_path
    local_path = os.path.join(os.path.dirname(os.path.dirname(CURRENT_DIR)), "models", MODEL_NAME.split('/')[-1])
    if os.path.exists(local_path): return local_path
    return MODEL_NAME

def build():
    model_path = get_model_path()
    print(f"ðŸš€ [Upgrade] Indexing with 1024D Model: {model_path}")
    model = SentenceTransformer(model_path)
    
    skills_data, descriptions = [], []

    for base_path in ASSETS_PATHS:
        if not os.path.exists(base_path):
            print(f"âš ï¸ Skip missing path: {base_path}")
            continue
            
        print(f"ðŸ“‚ Scanning assets in: {base_path}")
        for root, _, files in os.walk(base_path):
            # æ”¯æŒ SKILL.md (æ¡†æž¶) å’Œæ™®é€šçš„ .md (çŸ¥è¯†åº“æ–‡æ¡£)
            target_files = [f for f in files if f.endswith('.md')]
            
            for filename in target_files:
                path = os.path.join(root, filename)
                # æŽ’é™¤ README ç­‰éžæ ¸å¿ƒå†…å®¹
                if filename.lower() == 'readme.md' and "expert_frameworks" in path: continue
                
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ä¼˜å…ˆæå– YAML description æˆ–å¼€å¤´æ‘˜è¦
                    match = re.search(r'description:\s*(.*?)\n', content)
                    if match:
                        desc = match.group(1).strip()
                    else:
                        # æˆªå–å‰ 100 ä¸ªå­—ç¬¦ä½œä¸ºè¯­ä¹‰ç´¢å¼•é¢„è§ˆ
                        clean_content = re.sub(r'[#*`\-]', '', content[:300]).strip()
                        desc = clean_content.split('\n')[0][:150]
                    
                    rel_path = os.path.relpath(path, CURRENT_DIR)
                    name = f"[{os.path.basename(os.path.dirname(path))}] {filename}"
                    
                    skills_data.append({'name': name, 'path': rel_path, 'type': 'framework' if 'SKILL.md' in filename else 'knowledge'})
                    descriptions.append(f"{name}: {desc}")

    print(f"ðŸ§  Generating Embeddings for {len(descriptions)} assets...")
    embeddings = model.encode(descriptions, batch_size=16, show_progress_bar=True)
    
    with open(CACHE_FILE, 'wb') as f:
        pickle.dump({'metadata': skills_data, 'embeddings': embeddings, 'model_ver': MODEL_NAME}, f)
    print(f"âœ… Hybrid Index Completed: {len(descriptions)} assets -> {CACHE_FILE}")

if __name__ == "__main__": build()