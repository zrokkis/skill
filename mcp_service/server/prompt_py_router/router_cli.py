import os
import pickle
import re
from mcp.server.fastmcp import FastMCP
from sentence_transformers import SentenceTransformer, util

mcp = FastMCP("Prompt Router Service (V2 Optimized)")

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_FILE = os.path.join(CURRENT_DIR, "skills_cache.pkl")
# å‡çº§ä¸º 1024 ç»´åº¦æ¨¡å‹
MODEL_NAME = 'BAAI/bge-m3'

_model = None
_data = None

def get_model_path():
    env_path = os.environ.get("PEER_MODEL_PATH")
    if env_path and os.path.exists(env_path): return env_path
    local_path = os.path.join(os.path.dirname(os.path.dirname(CURRENT_DIR)), "models", MODEL_NAME.split('/')[-1])
    if os.path.exists(local_path): return local_path
    return MODEL_NAME

def get_resources():
    global _model, _data
    if _model is None:
        model_path = get_model_path()
        print(f"ğŸ“¦ Loading SOTA Model [1024D]: {model_path}")
        _model = SentenceTransformer(model_path)
    if _data is None:
        if not os.path.exists(CACHE_FILE):
            from ag_indexer import build
            build()
        with open(CACHE_FILE, 'rb') as f:
            _data = pickle.load(f)
    return _model, _data

@mcp.tool()
def search_skill(query: str, top_k: int = 3) -> str:
    """
    è¯­ä¹‰æ£€ç´¢ Expert æ¡†æ¶åŠç²¾é€‰çŸ¥è¯†åº“æ–‡æ¡£ã€‚
    æ”¯æŒè·¨åŸŸèµ„äº§è°ƒåº¦ã€‚
    """
    model, data = get_resources()
    query_embedding = model.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, data['embeddings'], top_k=top_k)
    top_results = hits[0]
    
    results = []
    for hit in top_results:
        score = hit['score']
        meta = data['metadata'][hit['corpus_id']]
        abs_path = os.path.abspath(os.path.join(CURRENT_DIR, meta['path']))
        asset_type = meta.get('type', 'unknown').upper()
        
        results.append(f"[{asset_type}] åˆ†æ•°: {score:.4f}\nåç§°: {meta['name']}\nè·¯å¾„: {abs_path}\n" + "-"*20)
    return "\n".join(results)

@mcp.tool()
def prompt(query: str) -> str:
    """
    åŸºäºæ··åˆç´¢å¼•è‡ªåŠ¨åŒ–ç”Ÿæˆå¢å¼ºæç¤ºè¯ã€‚
    å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯çŸ¥è¯†åº“æ–‡æ¡£ï¼Œå°†ä½œä¸º Context æ³¨å…¥ï¼›
    å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯ Frameworkï¼Œå°†ä½œä¸º Logic æŒ‡ä»¤æ³¨å…¥ã€‚
    """
    model, data = get_resources()
    query_embedding = model.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, data['embeddings'], top_k=1)
    best_hit = hits[0][0]
    meta = data['metadata'][best_hit['corpus_id']]
    
    prompt_file = os.path.abspath(os.path.join(CURRENT_DIR, meta['path']))
    if not os.path.exists(prompt_file):
        return f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°èµ„äº§æ–‡ä»¶ {prompt_file}"
        
    with open(prompt_file, 'r', encoding='utf-8') as f:
        full_content = f.read()
    
    is_framework = meta.get('type') == 'framework'
    
    optimized_response = f"""
### ğŸ¯ åŒ¹é…èµ„äº§ï¼š{meta['name']}
**åŒ¹é…ç½®ä¿¡åº¦**: {best_hit['score']:.4f}
**èµ„äº§ç±»å‹**: {"æ¡†æ¶é€»è¾‘ (Logic)" if is_framework else "èƒŒæ™¯çŸ¥è¯† (Context)"}

---

# ğŸš€ å¢å¼ºæç¤ºè¯ (Augmented Prompt)

ä½ ç°åœ¨çš„è§’è‰²æ˜¯ä¸€ä½å…·å¤‡æ·±åšèƒŒæ™¯çš„**è¡Œä¸šé¢†åŸŸä¸“å®¶**ã€‚è¯·åŸºäºä»¥ä¸‹{"æŒ‡ä»¤ä½“ç³»" if is_framework else "äº‹å®ä¾æ®"}ï¼Œå¤„ç†æˆ‘çš„æ ¸å¿ƒéœ€æ±‚ã€‚

### 1. æ ¸å¿ƒéœ€æ±‚ (User Query)
{query}

### 2. {"æ¡†æ¶æŒ‡ä»¤" if is_framework else "èƒŒæ™¯å‚è€ƒ"} (Expert Content)
{full_content}

---
**[PEER V2]**: ä»¥ä¸Šè¾“å‡ºå·²å®Œæˆè·¨åŸŸèµ„äº§å¯¹é½ã€‚
"""
    return optimized_response

if __name__ == "__main__":
    mcp.run()