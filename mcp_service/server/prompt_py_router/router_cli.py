
# Triggering reload for high-dimensional model
import os
import pickle
import re
from mcp.server.fastmcp import FastMCP
from sentence_transformers import SentenceTransformer, util

# åˆå§‹åŒ– MCP Server
mcp = FastMCP("Prompt Router Service")

# è·¯å¾„é…ç½®
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_FILE = os.path.join(CURRENT_DIR, "skills_cache.pkl")
MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'

# å…¨å±€å˜é‡ï¼Œæ‡’åŠ è½½
_model = None
_data = None

def get_resources():
    global _model, _data
    if _model is None:
        _model = SentenceTransformer(MODEL_NAME)
    if _data is None:
        if not os.path.exists(CACHE_FILE):
            print(f"âš ï¸ Index not found at {CACHE_FILE}. Building index now...")
            from ag_indexer import build
            build()
        with open(CACHE_FILE, 'rb') as f:
            _data = pickle.load(f)
    return _model, _data

@mcp.tool()
def search_skill(query: str, top_k: int = 3) -> str:
    """
    æ ¹æ®ç”¨æˆ·éœ€æ±‚è¯­ä¹‰ï¼Œä» 50+ ä¸ª Prompt æ¡†æ¶ä¸­æ£€ç´¢æœ€åŒ¹é…çš„æ¡†æ¶ã€‚
    è¾“å…¥ query ä¸ºç”¨æˆ·çš„åŸå§‹éœ€æ±‚ï¼ˆå¦‚ï¼šå¸®æˆ‘åˆ¶å®šè®¡åˆ’ã€æˆ‘æƒ³å†™è®®è®ºæ–‡ï¼‰ã€‚
    è¿”å›å€¼åŒ…å«åŒ¹é…åº¦ã€æ¡†æ¶åç§°åŠ SKILL.md è·¯å¾„ã€‚
    """
    model, data = get_resources()
    
    # 1. å‘é‡åŒ–æŸ¥è¯¢
    query_embedding = model.encode(query, convert_to_tensor=True)
    
    # 2. è¯­ä¹‰æœç´¢
    hits = util.semantic_search(query_embedding, data['embeddings'], top_k=top_k)
    top_results = hits[0]
    
    results = []
    for hit in top_results:
        score = hit['score']
        meta = data['metadata'][hit['corpus_id']]
        
        # å°è¯•æå–æè¿°é¢„è§ˆ
        desc = "No description available"
        if os.path.exists(meta['path']):
            with open(meta['path'], 'r', encoding='utf-8') as f:
                content = f.read()
                match = re.search(r'description:\s*(.*?)\n', content)
                if match:
                    desc = match.group(1).strip()
        
        results.append(f"åˆ†æ•°: {score:.4f}\nåç§°: {meta['name']}\nè·¯å¾„: {meta['path']}\næè¿°: {desc}\n" + "-"*20)
    
    return "\n".join(results)

@mcp.tool()
def prompt(query: str) -> str:
    """
    è¯­ä¹‰åŒ¹é…æœ€åˆé€‚çš„ Prompt æ¡†æ¶ï¼Œå¹¶æ ¹æ®è¯¥æ¡†æ¶çš„æ ¸å¿ƒé€»è¾‘ï¼Œå°†ç”¨æˆ·çš„è¾“å…¥è‡ªåŠ¨åŒ–ç¼–è¯‘ä¸ºé«˜é˜¶æç¤ºè¯ã€‚
    è¾“å…¥ query ä¸ºæ‚¨çš„åŸå§‹ä¸šåŠ¡æˆ–å†™ä½œéœ€æ±‚ã€‚
    è¿”å›å€¼æ˜¯ç»è¿‡æ¡†æ¶åŠ æŒåçš„ã€å³å¼€å³ç”¨çš„ç»ˆææç¤ºè¯ã€‚
    """
    model, data = get_resources()
    
    # 1. å¯»æ‰¾æœ€å¼ºåŒ¹é… (Top-1)
    query_embedding = model.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, data['embeddings'], top_k=1)
    best_hit = hits[0][0]
    meta = data['metadata'][best_hit['corpus_id']]
    
    # 2. è¯»å–æ¡†æ¶æ ¸å¿ƒè§„åˆ™
    prompt_file = meta['path']
    if not os.path.exists(prompt_file):
        return f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¡†æ¶æ–‡ä»¶ {prompt_file}"
        
    with open(prompt_file, 'r', encoding='utf-8') as f:
        full_content = f.read()
    
    # 3. æ„é€ ç¼–è¯‘åçš„å…¨é‡æç¤ºè¯ (Prompt Compilation)
    # æˆ‘ä»¬å°†æ¡†æ¶çš„æŒ‡ä»¤ä½“ç³»ä¸ç”¨æˆ·çš„åŸå§‹éœ€æ±‚è¿›è¡Œå¼ºç»‘å®š
    optimized_response = f"""
### ğŸ¯ æ¨èæ¡†æ¶ï¼š{meta['name'].replace('_', ' ').upper()}
**åŒ¹é…ç½®ä¿¡åº¦**: {best_hit['score']:.4f}

---

# ğŸš€ ä¼˜åŒ–åçš„æç¤ºè¯ (Copy & Paste below)

ä½ ç°åœ¨æ˜¯ä¸€ä½åœ¨è¯¥é¢†åŸŸå…·å¤‡æ·±åšé€ è¯£çš„**è¡Œä¸šä¸“å®¶**ã€‚è¯·åŸºäºä»¥ä¸‹ç»“æ„åŒ–æ¡†æ¶ï¼Œå¤„ç†æˆ‘çš„æ ¸å¿ƒéœ€æ±‚ã€‚

### 1. æ ¸å¿ƒä»»åŠ¡ (The Task)
{query}

### 2. æ‰§è¡Œé€»è¾‘ä¸æ€è€ƒå‡†åˆ™ (Framework Directives)
è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ç”± **[{meta['name']}]** æ¡†æ¶å®šä¹‰çš„æ‰§è¡Œæ ‡å‡†è¿›è¡Œè¾“å‡ºï¼š

{full_content}

---
**[ä½¿ç”¨è¯´æ˜]**: ä»¥ä¸Šå†…å®¹å·²ç”±äºå…¶å†…éƒ¨åŒ…å«å®Œæ•´çš„æ¡†æ¶é€»è¾‘ï¼Œè¯·ç›´æ¥å‘é€ç»™ AI å³å¯è·å¾—ä¸“å®¶çº§å“åº”ã€‚
"""
    return optimized_response

if __name__ == "__main__":
    mcp.run()